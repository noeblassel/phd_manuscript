\chapter{Introduction}
\label{chap:introduction}
In Section~\ref{sec:01:MD}, we introduce the broader scientific context of molecular dynamics simulations.
\section{An overview of molecular dynamics}
\label{sec:01:MD}
Molecular dynamics (MD) is a set of techniques aimed at extracting properties of atomistic systems from carefully designed computer simulations.
Due to the development of flexible and efficient methodologies, as well as the steady increase in available computational power over the last seventy years, MD has become a mainstay of computational physics, and is now routinely used in a variety of scientific applications from computational thermodynamics and material science to drug discovery and cell biology.
Fundamentally, MD simulates the time-evolution of systems at the atomic scale, by treating atoms as classical point particles, whose trajectories are governed by a defined set of interaction laws and dynamics. From the simulation and recording of these trajectories, several important scientific needs can be covered. Indeed, MD is simultaneously:
\begin{itemize}
\item \textit{A means to measure properties of matter}: MD provides numerical estimators for thermodynamic, structural and dynamical quantities that may be difficult or impossible to measure experimentally, due to extreme conditions or prohibitive costs. Typical outputs include radial distribution functions, free-energy differences, pressure and enthalpy, defect formation energies, reaction rates, and transport coefficients. With sufficiently long sampling, MD yields statistically converged values that can be used to parametrize coarser models. The computation of dynamical properties, and the associated need for long-time microscopic sampling, are a central motivation for Chapters~2--4.
  \item \textit{A numerical microscope}: MD simulations allows to resolve molecular trajectories at a level of detail which is far beyond the reach of physical experiments. The analysis of MD trajectories can help to visualize reaction pathways, identify transition states, observe collective rearrangements (such as nucleation, crystal defect propagation or fast protein folding), and extract mechanistic hypotheses that guide theoretical developments and experiments. These insights are especially valuable for understanding rare events and conformational changes, where a single trajectory can reveal the sequence of microscopic steps behind an observed macroscopic transition. MD simulations can be understood as \textit{in silico} experiments, which have become an important tool of modern materials and biological research.
  \item \textit{A benchmark for new methods}: due to its inherent flexibility, it is perhaps unsurprising that a major use case of MD is the development and testing of novel numerical and modelling tools for computational science at large, which in turn become useful for MD itself. New methods are validated on MD testbeds precisely because realistic atomistic models naturally combine the challenges of high dimensionality, nonlinearity and multiscale behavior.
  \item \textit{A hurdle for theory}: high-fidelity MD simulations have themselves become a source of “ground truth’’ data. Notably, long trajectories produced on bespoke hardware~\cite{SDDKLSYBBCal08} are routinely used to test biophysical hypotheses and to train data-driven models. At the same time, algorithms used in MD simulations expose some interesting mathematical questions, which are still a fruitful area of research, to which this thesis makes some contributions.
\end{itemize}
\noe{hyperrefs aux chapitres-- à ajouter}

\paragraph{Orders of magnitude}
{
Appreciating the vast discrepancies between the atomic, macroscopic and computational realms is key to understanding the scope of molecular dynamics simulations. It is therefore instructive to review some of the characteristic scales at play.

\noindent
\begin{minipage}[t]{0.48\textwidth}
\textbf{Atomistic scales}
\begin{itemize}
    \item Macroscopic amounts of matter are counted in moles of molecules, which are multiples of Avogadro's number~$N_{\mathrm{A}}=6.022\times 10^{23}$.
    \item Length is measured in units of {\AA ngstr\"oms}, ($1$~\AA$\,=10^{-10}\,\mathrm{m}$). For example, the Bohr radius of hydrogen is~$0.529$~\AA, while the helix of B-DNA has a diameter of~$20$~\AA.
    \item Time is measured in units of femtoseconds,($1$\,fs$=10^{-15}$\,s), which is also the typical timestep for MD simulations. The fastest molecular vibrational periods (for instance those associated with hydrogen bond stretching modes) span the order of~$10\,\mathrm{fs}$.
\end{itemize}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\textwidth}
\textbf{Computational scales}
\begin{itemize}
  \item A 2018 projection~\cite{RGR18} estimates the size of the global ``datasphere'' (the total amount of digital information on Earth) to be reach $175$~zettabytes by 2025, which corresponds to~$0.29\,N_{\mathrm{A}}$~bytes.
  \item Typical consumer machines provide on the order of~$10^{12}$~bytes of storage capacity, whereas data centers and high-performance computing facilities operate at the~$10^{15}$--$10^{18}$ bytes scale.
  \item Consumer CPUs and GPUs deliver on the order of~$10^{9}$--$10^{12}$ floating-point operations by second (FLOPS), while modern exascale machines target sustained performance at around~$10^{18}$~FLOPS~\cite{Sc22}.
\end{itemize}
\end{minipage}
}
\newline\newline
\noindent   
This profound disparity between computational resources and atomic scales implies that MD falls short of fully resolving the microscopic trajectories of macroscopic systems. More critically, it reveals a fundamental bottleneck for observing phenomena of scientific interest. With timesteps on the order of femtoseconds, simulating even a single microsecond of a system's evolution requires executing on the order of a billion sequential steps. To compound this difficulty, the floating-point operation cost of a single timestep typically scales linearly with the number of atoms in the system.
Even with modern GPUs delivering trillions of operations per second, this fact imposes a hard limit on the total~$\text{simulation length }\times{ system size}$. While acceleration in the spatial domain is achievable using parallel architectures and domain decomposition schemes, the sequential nature of trajectories prevents a similar acceleration in the time domain. For a typical solvated protein, a full day of computation on a high-performance GPU yields, at best, a few hundred nanoseconds of trajectory time~\cite{HD18}, and much less for the large systems of interest to materials science. Yet, many important processes are observed to unfold over microseconds to seconds timescales, or even longer.

\paragraph{Some historical milestones.}
\label{par:01:milestones}
Despite these inherent limitations, full-atom ``brute-force'' MD simulations have been successfully applied to various problems. Some simulations, notable for their historical importance and/or their novel magnitude, are listed below.
\begin{itemize}
    \item{\textit{1953}: Metropolis et al.~\cite{MRTT53} compute by the Monte--Carlo method an equation of state for a hard-sphere model, spawning the field of computational statistical physics. This early work is followed in \textit{1957} by the first simulation~\cite{AWal57} of the molecular dynamics of a hard sphere model.}
    \item{\textit{1964}: Rahman~\cite{R64} measures properties of liquid argon using a MD simulation of interacting Lennard--Jones particles. Results are consistent with experimental measurements. This is followed in~\textit{1971} by the more challenging case of liquid water~\cite{RS71} by Rahman and Stillinger.}
    \item{\textit{1975}: Levitt and Warshel~\cite{LW75} simulate a protein folding using a coarse-grained energy minimization procedure.}
    \item{\textit{1988}: Levitt and Sharon~\cite{LS88} perform the first simulation of a protein in explicit water solvent, a~$0.2$\,nanosecond-long trajectory.}
    \item{\textit{1998}: Duan and Kollman~\cite{DK98} publish the first microsecond-long simulation of a fast-folding protein in explicit solvent, the villin headpiece, exposing the intricate mechanisms underlying protein folding.}
    \item{\textit{2002}: Abraham et al.~\cite{AWGDDDLRS02} carry out the first MD simulation involving more than a billion atoms, a short trajectory of a flawed FCC crystal undergoing ductile failure.}
    \item{\textit{2008}: Germann and Kadau~\cite{GK08} report the first MD simulation involving a trillion atoms, 40 timesteps of a Lennard--Jones crystal.}
    \item{\textit{2010}: Shaw et al.~\cite{SMLLPDEBJSSal10} perform the first millisecond-long simulation of a protein in explicit solvent, at great cost, using a dedicated machine design~\cite{SDDKLSYBBCal08}.}
    \item{\textit{2017}: Perilla and Schulten~\cite{PS17} publish a full-atom simulation of the HIV-1 viral capsid in water (64 million atoms for~$1.2\,\mu$s).}
\end{itemize}

MD simulations provide valuable insight into the thermodynamic and kinetic properties of molecular system, using methods rooted in statistical mechanics. 

\subsection{Elements of statistical mechanics}
Statistical mechanics is the rigorous attempt to reconcile the microscopic point of view, in which a system's many microscopic degrees of freedom evolve according to fundamental physical laws, and the macroscopic point of view, according to which only a handful of variables are relevant to describe the system's state and evolution.
Here, we present the necessary formalism to treat the molecular systems of interest in MD. In particular, we restrict our scope to classical systems.
We note however that a similar Gibbsian formalism also exists for quantum systems (see~\cite{F72}) and has been more generally used to great effect in the study of a variety of disordered systems, such as spin glasses~\cite{EA75}, Hopfield networks~\cite{P84} and their quantum counterparts~\cite{RY96,RMGLM18}.

\paragraph{Microscopic states, their energy and classical dynamics.}
In this thesis, we consider systems of $N>0$ point particles, representing classical atomic nuclei.
The system's microscopic configuration, or \textit{microstate}, is described by the positions and momenta of each one of these nuclei. The microstate therefore corresponds to a point in \textit{phase space}, 
\begin{equation}
    \label{eq:01:phase_space}
    (q,p)\in \cE := \cX\times \cM,
\end{equation}
where~$\cX$ is a configurational domain, and for a configuration~$(q,p)\in\cE$, $q\in\cX$ is the position variable, and~$p\in \cM$ is the associated momentum variable in the momentum space~$\cM$.

To each microstate~$(q,p)\in\cE$, we associate its energy~$H(q,p)$. The function~$H$ is called the~\textit{Hamiltonian}. In most situations,~$\cM=\R^{3N}$, and the Hamiltonian takes the separable form
\begin{equation}
    \label{eq:01:hamiltonian}
    H(q,p) = V(q) + \frac12p^\top M^{-1}p,\qquad M = \begin{pmatrix}
    m_1 \I_3 & 0 & \dotsm & 0 \\
    0 & m_2 \I_3 & \dotsm & 0 \\
    \vdots & \ddots & \ddots & \vdots\\
    & \dotsm & 0 & m_N \I_3 
\end{pmatrix}
\end{equation}
where~$V:\cX\to\R$ is a potential energy function, the term~$\frac12p^\top M^{-1}p$ gives the kinetic energy, and~$M$ is a diagonal matrix encoding the atomic masses in the system, with~$m_i>0$ giving the mass of the~$i$-th particle for~$1\leq i\leq N$.

The classical equations of motion, as described by Newton's second law, can then be written compactly using the Hamiltonian~\eqref{eq:01:hamiltonian}. They are equivalently expressed by the following ordinary differential equation in~$\cE$:
\begin{equation}
    \label{eq:01:hamiltonian_dynamics}
    \frac{\d}{\d t}\,X_t = -J\nabla H(X_t),\qquad X_t = (q_t,p_t)\in \cE,
\end{equation}
where~$J$ is the~\textit{symplectic matrix}
\begin{equation}
    \label{eq:01:J}
    J=\begin{pmatrix}
        0&\I_{3N}\\-\I_{3N}&0
    \end{pmatrix}.
\end{equation}
In this form, the equation~\eqref{eq:01:hamiltonian_dynamics} is known as~\textit{Hamiltonian dynamics}, and its trajectories in phase space describe the time-evolution of an isolated system of classical particles.

\paragraph{On the choice of the interaction potential~$V$.}
The potential~$V$ is the crucial physical parameter, as it encodes the interactions between nuclei, and therefore their dynamics. Ideally, it is defined by the ground-state energy of the electronic Schr\"odinger operator in the Borne--Oppenheimer approximation~\cite{BO27}:
\begin{equation}
    \label{eq:01:schrodinger}
    V_{\mathrm{BO}}(q) = \inf\left\{\left\langle\psi, H(q) \psi\right\rangle_{\mathcal H_\cX}:\,\psi \in \mathcal H^1_{\cX,q},\,\|\psi\|_{\mathcal H_\cX}=1\right\},
\end{equation}
where~$\mathcal H_{\cX}$ and~$H(q)$ are respectively the Hilbert space of electronic wave functions and the electronic Hamiltonian associated with a given position of classical nuclei~$q\in\cX$, and~$\mathcal H^1_{\cX,q}\subset \mathcal H_{\cX}$ is the associated form domain.

For systems of interest in MD, the problem~\eqref{eq:01:schrodinger} is a very high-dimensional partial differential equation (PDE) eigenvalue problem, and therefore cannot be solved directly. Instead, one resorts to a variety of approximations to approximate~$V_{\mathrm{BO}}$.
Since the dynamics~\eqref{eq:01:hamiltonian_dynamics} only depends on the force~$\nabla V$, these classes of approximations are also known as~\textit{force fields}, of which we distinguish three main families.
\begin{itemize}
    \item{\textit{Ab initio methods} leverage the extensive progress in electronic structure theory over the last 100 years. Many numerical methods have been developed to address the problem~\eqref{eq:01:schrodinger}: with no claim of exhaustivity, popular schemes include the Hartree--Fock method, the more precise (and costly) post--Hartree--Fock methods, as well as methods rooted in density functional theory (DFT), see~\cite{J99} for a comprehensive introduction. While such methods are very accurate, as they incorporate effects of the electronic structure on the nuclear dynamics, their poor scaling with respect to system size and their high cost of evaluation limit their applications to small systems and short simulation times. They are nevertheless essential to capture some phenomena, such as bond-breaking in chemical reactions.}
    \item{\textit{Empirical force fields}, which constitute the most important class of methods historically, proceed by first selecting a set of prototypical systems~$\mathcal S$, and for each~$s\in\mathcal S$, introducing a parametric ansatz~$V^{s}_\theta(q)$ for the minimum~\eqref{eq:01:schrodinger}. The functional form of~$V^s_\theta$ is hand-crafted by finding a balance between physical priors and computational efficiency concerns. A crucial point is that the functional form should be defined unambiguously for systems~$s'\not\in\mathcal S$, to allow for extrapolation. A regression can then be performed to select an optimal parameter~$\theta^*$, in order to replicate a set of targeted structural properties over~$\mathcal S$, measured either experimentally or using ab initio computations. The empirical potential~$V_{\theta^*}^{s'}$ can then be used to probe the system~$s'\not\in\mathcal S$.
    The oldest and simplest example of empirical potential is the Lennard--Jones pair-potential~\eqref{eq:01:lennard_jones}, which only uses two parameters, but many families of force-fields of this type are still widely used, such as CHARMM~\cite{BBOSSK83},~AMBER~\cite{PCCRCDFSK95} and GROMOS~\cite{SHTMBFTHKVG99} for biomolecules, EAM~\cite{DB84} for metallic systems, or Tersoff-type potentials~\cite{T89} for multi-species solids.}
    \item{\textit{Machine-learned interatomic potentials} (MLIP) can be understood as the application of modern machine-learning architectures in the empirical approach described above. While they are conceptually the same, the two approaches nevertheless differ in that the parameters no longer have any clear physical meaning. MLIPs has recently gathered interest, due to the hope of nearly matching the accuracy of ab-initio methods at a fraction of their cost. This promise has gained some credence in the past few years, due to the demonstrated flexibility of neural-network architectures in other fields of computational science, and to the rapid increase in the availability of ab-initio training data.
    A common feature of these methods is to first represent the local atomic environment of each atom by a set of descriptors, or~\textit{symmetry functions}, which are invariant by translation, rotation, and permutation of identical atoms, and to then use these descriptors as inputs to a machine learning model which outputs atomic energies. Popular architectures include Behler-Parrinello Neural Networks (BPNN)~\cite{BP07}, Gaussian Approximation Potentials (GAP)~\cite{BKKM10} which are based on kernel-ridge regression, and more recently deep neural network architectures such as ANI~\cite{SIBR17} or SchNet~\cite{SKT+17}. A major challenge for the development of such methods remains the generation of large, diverse and high-quality training datasets, which is itself an active area of research~\cite{GET+20}.
    } % @Gemini expand here
\end{itemize}

The AMBER family of force-fields, like most biomolecular empirical force fields, has a functional form which decomposes the potential energy into a sum of terms associated with covalent bonds, bond angles, dihedral angles, and non-bonded interactions, which include both van der Waals and electrostatic forces. A typical expression is:
\begin{equation}
\begin{split}
    \label{eq:01:amber}
    V(q) = &\sum_{\text{bonds}} \frac{k_b}{2}(l-l_0)^2 + \sum_{\text{angles}} \frac{k_a}{2}(\theta-\theta_0)^2 \\
    &+ \sum_{\text{dihedrals}} \frac{V_n}{2}\left(1+\cos(n\phi-\delta)\right) \\
    &+ \sum_{i<j}\left(4\varepsilon_{ij}\left[\left(\frac{\sigma_{ij}}{r_{ij}}\right)^{12}-\left(\frac{\sigma_{ij}}{r_{ij}}\right)^6\right] + \frac{Q_iQ_j}{4\pi\epsilon_0 r_{ij}}\right),
\end{split}
\end{equation}
where the first sum runs over pairs of covalently bonded atoms, the second over triplets of atoms forming an angle, and the third over quartets of atoms defining a dihedral angle. The final sum runs over all pairs of atoms~$i,j$ that are not involved in the same bond or angle (or are separated by a fixed number of bonds, e.g., 3, depending on the specific implementation) to avoid double-counting. The parameters~$\{k_b,l_0,k_a,\theta_0,V_n,n,\delta,\varepsilon_{ij},\sigma_{ij},Q_i\}$ are specific to atom types and are meticulously parametrized to reproduce experimental and quantum mechanical data.

The Lennard--Jones potential has the form
\begin{equation}
    \label{eq:01:lennard_jones}
    V(q) = \sum_{1\leq i < j \leq N} V_{\mathrm{LJ}}\left(|q_i-q_j|\right),\qquad V_{\mathrm{LJ}}(r) = 4\varepsilon \left[\left(\frac{\sigma}{r}\right)^{12}-\left(\frac{\sigma}{r}\right)^6\right].
\end{equation}

\paragraph{Boundary conditions.}
The specific definition of the phase space~$\cE$ depends on the physical model and properties of interest. We distinguish several common choices,  listed here in approximate order of frequency of use in MD simulations.
\begin{itemize}
    \item{\textit{Periodic boundary conditions}: this common choice corresponds to setting~$\cX = (L\T)^d$ and~$\cM=\R^d$, where~$\T=\R/\Z$ is the unit one-dimensional torus,~$d=3N$ is the number of position degrees of freedom in the system, and~$L>0$ is a length parameter fixing the size of the domain. This setup is essential for studying the bulk properties of matter, as the periodic unit cell models a small portion of the molecular medium while its images represent the surrounding environment, eliminating surface effects.}
    \item{\textit{Non-flat position manifolds}: for certain applications, it is useful to restrict the particle positions to a non-flat manifold~$\cX$. This can be useful for enforcing geometric constraints, such as fixed bond length or angles, or for expressing the equations of motion in non-Cartesian coordinates~\cite{VJ15}. Such constraints can improve the stability of numerical schemes~\cite{RCB77,A83,BKLS95}, allowing for larger time steps, and can be used for computing free energy differences~\cite{SC98,LRS12}. In this geometric setting, the momentum associated to a given position~$q\in\cX$ is a cotangent vector~$p\in T_q^*\cX$, and the phase space~$\cE$ is the cotangent bundle:~$\cE=T^*\cX$.}
    \item{\textit{Unbounded domains}: setting~$\cX$ and~$\cM$ equal to~$\R^d$ is appropriate for studying molecular systems in isolation, such as small atomic clusters or single molecules in vacuum.}
    \item{\textit{Exotic boundary conditions}: for the purpose of some specialized simulations, one can consider a variety of additional boundary conditions. For instance, one can consider walls at the boundary of a domain~$\partial\cX$, on which particles are subject to specular or diffuse reflections, or absorption~(the role of absorbing boundaries is crucial in Chapters~2 and~3 of this thesis). Mixed conditions, which are periodic with respect to a subset of coordinates, can be used to study interfaces. One can even consider time-dependent definitions, such as Lees--Edwards boundary conditions~\cite{LE72}, which allow to study shear flows.}
\end{itemize}

\paragraph{Statistical ensembles.}
The basic postulate of statistical mechanics, as formalized by Gibbs in~\cite{G02}, is that the system's macroscopic configuration is a probability distribution~${\pi\in\cP(\cE)}$ over the set of possible microstates.
The distribution~$\pi$ is also known as a statistical or~\textit{thermodynamic ensemble}. In this statistical description, the ensemble~$\pi$ assigns to each microstate the likelihood of underlying the observed macrostate, and is therefore a crucial model parameter.

Given a physical observable~$\varphi:\cE\to\R$, we can then define the macroscopic value of~$\varphi$ as the~\textit{ensemble average}:
\begin{equation}
    \label{eq:01:ensemble_average}
    \langle \varphi\rangle_{\pi} = \E_\pi\left[\varphi\right]= \int_{\cE}\varphi\,\d \pi.
\end{equation}

To make the description operational, one should seek a principled way to assign a specific ensemble to a given set of physical conditions.
We distinguish two families of methods to satisfy this goal.

\begin{itemize}
    \item{\textit{Dynamical definitions}. In these constructions, the ensemble~$\pi$ is identified with an invariant probability measure or~\textit{steady state} of the system's underlying dynamics.
    Whether the dynamics are deterministic or stochastic, whenever the system is ergodic with respect to~$\pi$, time averages of observables along a single, long trajectory will converge to the ensemble average~\eqref{eq:01:ensemble_average}.
    This approach provides a physical justification for the ensemble by connecting it directly to the microscopic time-evolution of the system, in the case a model of the microscopic dynamics is available.
    It also the viewpoint which constitutes the theoretical underpinning the computation of thermodynamic quantities in equilibrium and nonequilibrium molecular dynamics.
    }
    \item{Variational definitions based on the \textit{principle of maximal entropy}, as described in \cite{J57a,J57b}, offer a fairly general alternative.
    This approach frames the problem of determining the ensemble from the knowledge of macroscopic data as one of statistical inference.
    Namely, the ensemble~$\pi$ is defined as the probability distribution which maximizes the information entropy, given by the ensemble average $S[\pi] = -\langle\log\pi\rangle_\pi$ (with some abuse of notation), subject to a set of constraints derived from the observation of the values of a number of macroscopic variables.
    Informally, this principle selects the most uninformative distribution compatible with the available information, and draws a connection between statistical mechanics and information theory. Solving for~$\pi$ generally leads to explicit expressions, independently of any hypotheses concerning the microscopic dynamics.}
\end{itemize}

\paragraph{Examples of statistical ensembles.}
We now give some examples, the first two of which were introduced by Gibbs in~\cite{G02}, and are of primary interest for MD.
\begin{itemize}
    \item{\textit{The microcanonical ensemble} ($NVE$) describes an isolated system with a fixed number of particles ($N$), volume ($V$), and total energy ($E$). It is given by the probability measure~$\mu_{NVE}$, where
    \begin{equation}
        \label{eq:01:microcanonical_distribution}
        \forall\,A\in\mathcal B(\cE),\qquad \mu_{NVE}(A) = \frac1{Z_{NVE}}\int_{A\cap H^{-1}(E)}|\nabla H|^{-1}\d\mathcal{H}_{H^{-1}(E)},
    \end{equation}
    where~$\mathcal{H}_{H^{-1}(E)}$ is the~$(d-1)$-dimensional Hausdorff measure on the constant-energy surface~$H^{-1}(E)$ and
    \[Z_{NVE}=\int_{H^{-1}(E)}|\nabla H|^{-1}\d\mathcal{H}_{H^{-1}(E)}\]
    is a normalizing constant known as the~\textit{microcanonical partition function}. Dynamically, it is the invariant measure of Hamiltonian dynamics~\eqref{eq:01:hamiltonian_dynamics} started at a point~$X_0\in H^{-1}(E)$, assuming the ergodic hypothesis. The measure~$\mu_{NVE}$ can also be viewed as the limit as~$\delta\to 0$ of uniform probability distributions on the energy shells~$S(E,\delta):=H^{-1}(E-\delta,E+\delta)$, which are well-known to maximize the information entropy in~$\cP(S(E,\delta))$ whenever~$S(E,\delta)$ is compact.}

    \item{\textit{The canonical ensemble} ($NVT$) describes a system with fixed number of particles and volume, in thermal equilibrium with a heat bath at a constant temperature $T$. It is given by the \textit{Boltzmann--Gibbs distribution}
    \begin{equation}
        \label{eq:01:boltzmann_gibbs}
        \forall\,A\in\mathcal B(\cE),\qquad \mu(A) = \frac{1}{Z_{NVT}(\beta)}\int_{A}\e^{-\beta H(q,p)}\,\d q\,\d p,
    \end{equation}
    where~$H$ is defined in~\eqref{eq:01:hamiltonian},~$\beta = (k_{\mathrm{B}}T)^{-1}$,~$k_{\mathrm{B}}$ is Boltzmann's constant and
    \[
    Z_{NVT}(\beta) = \int_{\cE}\e^{-\beta H(q,p)}\,\d q\,\d p
    \]
    is the~\textit{canonical partition function}.
    It correspond to invariant probability measure for a system evolving under various stochastic dynamics (some of which are discussed in Section~\ref{subsec:01:sampling} below) which model the interaction with the heat bath. From the point of view of the maximum entropy principle, it is derived by maximizing the information entropy subject to the constraint of a fixed \textit{average} energy, which is related to the temperature $T$ by the formula~${\langle H\rangle_\mu = -\frac{\partial}{\partial\beta}Z_{NVT}(\beta)}$. }
    \item{Other equilibrium ensembles can be constructed to model more general physical conditions. The \textit{isothermal-isobaric ensemble} ($NPT$) describes systems at constant temperature and pressure~$P$, which are sometimes relevant for biological applications, and is derived from the maximal entropy principle by fixing the average energy and average volume of the system (related to the pressure~$P$). The \textit{grand-canonical ensemble} ($\mu$VT) describes systems that can exchange both heat and particles with a bath, and is defined by fixing the average energy and average particle number (related to the chemical potential $\mu$).}
    \item{\textit{Nonequilibrium ensembles}, describe systems driven away from thermal equilibrium by the application of non-conservative forces or thermal gradients. These systems are characterized by the presence of irreversible fluxes and entropy production. Most often, these ensembles are defined dynamically, as the invariant measure of some nonequilibrium process, in which case the ensemble is also known as a nonequilibrium steady-states~(NESS). The question of finding variational constructions for nonequilibrium ensembles has been investigated in the physical literature~(see~\cite{J80}), but does not appear to be fully settled at this time.}
\end{itemize}
Finally, let us mention that \textit{equivalence of ensembles} results allow to relate averages in one ensemble to averages in another. For instance, it has been shown that, for systems with short-range interactions, the~$NVE$ and~$NVT$ ensembles are equivalent (in several ways, and under technical conditions, see~\cite{T15} for a detailed discussion) in the thermodynamic limit~$N,V\to +\infty$ keeping the particle density~$\rho = N/V$ fixed.
In particular, so called~\textit{macrostate equivalence} results imply that, for intensive observables~$\varphi$, the canonical averages~$\langle\varphi\rangle_{NVT}$  and corresponding microcanonical averages $\langle \varphi\rangle_{NVE}$ converge to a common limit when~$N\to +\infty$, where~$T$ and~${E=Nu}$ are chosen so that the canonical specific energy~$\langle H/N\rangle_{NVT}$ converges to the microcanonical one~$u$.
Equivalence results for nonequilibrium ensembles is also a current topic of interest, see~\cite{CT13} for an example.

\subsection{Sampling equilibrium properties}
\label{subsec:01:sampling}

\paragraph{The Monte--Carlo method.}

\paragraph{Langevin dynamics.}
- Formula for underdamped Langevin dynamics
- Historical reference of derivation from coupled oscillators (Ford, Kac, Mazur)
- Use for sampling Boltzmann--Gibbs
- Overdamped limit

\paragraph{Other dynamics for canonical sampling.}
- More general diffusion and frictions coefficients
- PDMPS
- Nosé--Hoover chains
- Adaptive

\paragraph{Collective variables and free-energy computations.}

\paragraph{Enhanced sampling.}

\subsection{Sampling dynamical properties}
- State-to-state transition statistics
- Transition states
- Transport coefficients

\subsection{Challenges in MD}
- Metastability
- The timescale problem
- Response properties

\section{Mathematical approaches to the problem of metastability}
\label{sec:01:metastability}
\subsection{Global descriptions}
\subsection{Local descriptions}
\subsection{Numerical methods}

\section{Main contributions of this thesis}
\label{sec:01:contributions}
\paragraph{On the mathematical study of metastability}
\paragraph{Accelerated MD methodology}
\paragraph{Nonequilibrium sampling}